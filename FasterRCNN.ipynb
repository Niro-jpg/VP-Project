{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "o8nd-4w1ECvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqaET_PtEAP7",
        "outputId": "30ae14bd-5dfa-480f-c942-9b8ecbce96e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tFjHilt6EGZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "GXBoOVc_EWzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = torchvision.models.resnet50(pretrained=True)\n",
        "        req_layers = list(model.children())[:9]\n",
        "        self.backbone = nn.Sequential(*req_layers)\n",
        "        for param in self.backbone.named_parameters():\n",
        "            param[1].requires_grad = True\n",
        "\n",
        "    def forward(self, img_data):\n",
        "        return self.backbone(img_data)\n",
        "\n",
        "\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super(RPN, self).__init__()\n",
        "\n",
        "        self.conv_obj = nn.Conv2d(in_channels, num_anchors, kernel_size=3, padding=1)\n",
        "        self.conv_bbox = nn.Conv2d(in_channels, 4 * num_anchors, kernel_size=3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B=x.shape[0]\n",
        "        obj_scores = self.conv_obj(x)\n",
        "        obj_scores=obj_scores.reshape(B,-1)\n",
        "\n",
        "        bbox_preds = self.conv_bbox(x)\n",
        "        bbox_preds=bbox_preds.reshape(B,-1,4)\n",
        "\n",
        "        return obj_scores,bbox_preds\n",
        "\n",
        "\n",
        "\n",
        "class FastRCNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FastRCNN1, self).__init__()\n",
        "        self.conv2 = nn.Conv2d(6, 6, 5)\n",
        "        self.fcc1 = nn.Linear(22326, 2048)\n",
        "        self.fcc2 = nn.Linear(2048, 1024)\n",
        "        self.fcc3 = nn.Linear(1024, 256)\n",
        "        self.fcc4 = nn.Linear(256, 64)\n",
        "        self.fcc5 = nn.Linear(64, 4*9)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(64, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.cls_score = nn.Linear(512, 9*4)\n",
        "        self.bbox_pred = nn.Linear(512, 9 * 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.adaptive_max_pool2d(x, (8,8))\n",
        "        x = x.reshape(24,-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        class_scores = self.cls_score(x)\n",
        "\n",
        "        class_scores = class_scores.reshape(-1,4)\n",
        "        bbox_predictions = self.bbox_pred(x)\n",
        "        bbox_predictions=bbox_predictions.reshape(-1,9,4)\n",
        "\n",
        "\n",
        "        return class_scores, bbox_predictions\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, box,labels,transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.box=box\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        box=self.box[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, box,label\n",
        "\n",
        "\n",
        "\n",
        "def prepare_dataset(path,img_size):\n",
        "  p=sorted(os.listdir(path))\n",
        "  labels_map = {'apple':0,'banana':1,'orange':2,'back':3}\n",
        "  images=torch.tensor([])\n",
        "  boxes=torch.tensor([])\n",
        "  classes=torch.tensor([])\n",
        "  for i in range(0,len(p),2):\n",
        "    image=cv2.imread(path+p[i])\n",
        "    scale_x=img_size/image.shape[1]\n",
        "    scale_y=img_size/image.shape[0]\n",
        "    image = cv2.resize(image, (img_size,img_size))\n",
        "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    img=torch.from_numpy(image.transpose(2,0,1)).float().unsqueeze(0)/255\n",
        "    images=torch.cat([images,img],dim=0)\n",
        "\n",
        "    tree = ET.parse(path+p[i+1])\n",
        "    root = tree.getroot()\n",
        "\n",
        "    xmin = root.findall(\"object/bndbox/xmin\")\n",
        "    xmin = [round(int(x.text)*scale_x) for x in xmin]\n",
        "    ymin = root.findall(\"object/bndbox/ymin\")\n",
        "    ymin = [round(int(y.text)*scale_y) for y in ymin]\n",
        "    xmax = root.findall(\"object/bndbox/xmax\")\n",
        "    xmax = [round(int(x.text)*scale_x) for x in xmax]\n",
        "    ymax = root.findall(\"object/bndbox/ymax\")\n",
        "    ymax = [round(int(y.text)*scale_y) for y in ymax]\n",
        "    box=[[item1,item2,item3,item4] for item1, item2, item3, item4 in zip(xmin,ymin,xmax,ymax)]\n",
        "    while(len(box)<9):\n",
        "      box.append([0,0,0,0])\n",
        "    box=torch.tensor(box).unsqueeze(0)\n",
        "    boxes=torch.cat([boxes,box],dim=0)\n",
        "\n",
        "\n",
        "    names=root.findall('object/name')\n",
        "    cls=torch.tensor([3,3,3,3,3,3,3,3,3])\n",
        "    i=0\n",
        "    for name in names:\n",
        "      cls[i]=labels_map[name.text]\n",
        "      i=i+1\n",
        "    cls=cls.unsqueeze(0)\n",
        "    classes=torch.cat([classes,cls],dim=0)\n",
        "  return images, boxes, classes\n",
        "\n",
        "\n",
        "def anchors():\n",
        "  anchor_boxes=[]\n",
        "  ratios=torch.tensor([0.33,1,3])\n",
        "  xc=[32,64,96,128,160,190]\n",
        "  yc=xc\n",
        "  for x in xc:\n",
        "    for y in yc:\n",
        "      for width in [45,72,100,140,200]:\n",
        "        for ratio in ratios:\n",
        "          width=width/torch.sqrt(ratio)\n",
        "          heigth=ratio*width\n",
        "          anchor_boxes.append([max(x - width / 2,0), max(y - heigth / 2,0), min(x + width / 2,image_size), min(y + heigth / 2,image_size)])\n",
        "  return torch.tensor(anchor_boxes).int()\n",
        "\n",
        "\n",
        "def build_target(t,tresh):\n",
        "  max_values, _ = torch.max(t, dim=2)\n",
        "  v = (max_values > tresh).float()\n",
        "  return v\n",
        "\n",
        "\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    if box1.numel() == 0 or box2.numel() == 0:\n",
        "        return 0.0\n",
        "    if box1.dim() == 0 or box2.dim() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    x1 = max(box1[0].item(), box2[0].item())\n",
        "    y1 = max(box1[1].item(), box2[1].item())\n",
        "    x2 = min(box1[2].item(), box2[2].item())\n",
        "    y2 = min(box1[3].item(), box2[3].item())\n",
        "\n",
        "    if x1 >= x2 or y1 >= y2:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    box1_area = (box1[2].item() - box1[0].item()) * (box1[3].item() - box1[1].item())\n",
        "    box2_area = (box2[2].item() - box2[0].item()) * (box2[3].item() - box2[1].item())\n",
        "\n",
        "    iou = intersection_area / (box1_area + box2_area - intersection_area)\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "\n",
        "def get_iou_mat(images, anchors, boxes):\n",
        "    B = images.shape[0]\n",
        "    N = boxes.shape[1]\n",
        "    nanc = anchors.shape[0]\n",
        "    ious_mat = torch.zeros((B, nanc, N))\n",
        "\n",
        "    for i in range(B):\n",
        "        for j in range(nanc):\n",
        "            for k in range(N):\n",
        "                ious_mat[i, j, k] = calculate_iou(anchors[j], boxes[i][k])\n",
        "\n",
        "    return ious_mat\n",
        "\n",
        "\n",
        "\n",
        "def compute_targets(images, anchors, boxes):\n",
        "    B=images.shape[0]\n",
        "    mat=get_iou_mat(images, anchors, boxes)\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    cls_targets = build_target(mat,0.6)\n",
        "    reg_targets = torch.zeros((B,num_anchors, 4), dtype=torch.float32)\n",
        "    indexes=torch.argmax(mat, dim=2)\n",
        "\n",
        "    for b in range(B):\n",
        "      for i, anchor in enumerate(anchors):\n",
        "        if cls_targets[b][i] == 1:\n",
        "             index=indexes[b][i]\n",
        "             dx = (boxes[b][index][0] - anchor[0]).item()\n",
        "             dy = (boxes[b][index][1] - anchor[1]).item()\n",
        "             dX = (boxes[b][index][2] - anchor[2]).item()\n",
        "             dY = (boxes[b][index][3] - anchor[3]).item()\n",
        "             reg_targets[b][i]= torch.tensor([dx, dy, dX, dY], dtype=torch.float32)\n",
        "\n",
        "\n",
        "    return cls_targets, reg_targets\n",
        "\n",
        "\n",
        "\n",
        "def get_proposals(out1,out2):\n",
        "  B=out2.shape[0]\n",
        "  N=out2.shape[1]\n",
        "  indices=torch.where(torch.sigmoid(out1)>0.72)\n",
        "  box_cor=torch.zeros(B,N,4)\n",
        "  for j in range(B):\n",
        "    for i in range(N):\n",
        "\n",
        "      box_cor[j][i][0]=anchors[i][0]+out2[j][i][0]\n",
        "      box_cor[j][i][1]=anchors[i][1]+out2[j][i][1]\n",
        "      box_cor[j][i][2]=anchors[i][2]+out2[j][i][2]\n",
        "      box_cor[j][i][3]=anchors[i][3]+out2[j][i][3]\n",
        "\n",
        "  proposals=torch.zeros(B,1,4)\n",
        "  flags=[0]*24\n",
        "  for i in range(len(indices[0])):\n",
        "    x,y=indices[0][i], indices[1][i]\n",
        "    new_box = box_cor[x][y]\n",
        "    iou = torch.tensor([calculate_iou(new_box, proposals[x][j]) for j in range(flags[x])])\n",
        "    if (iou < 0.7).all():\n",
        "      if flags[x] >= proposals.shape[1]:\n",
        "              new_proposals = torch.zeros(proposals.shape[0], proposals.shape[1] + 1, 4)\n",
        "              new_proposals[:, :proposals.shape[1], :] = proposals\n",
        "              proposals = new_proposals\n",
        "\n",
        "      proposals[x][flags[x]]=box_cor[x][y]\n",
        "      flags[x]=flags[x]+1\n",
        "\n",
        "  return proposals"
      ],
      "metadata": {
        "id": "F5NVupkREch0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FasterRCNN"
      ],
      "metadata": {
        "id": "jmHrhTR_Eyuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='drive/MyDrive/train_zip/train/'\n",
        "image_size=256\n",
        "images,boxes,classes=prepare_dataset(path,image_size)\n",
        "dataset=CustomDataset(images,boxes,classes)"
      ],
      "metadata": {
        "id": "h7JVRxhvE3Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 24\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "images_b,boxes_b,classes_b=next(iter(dataloader))\n",
        "anchors=anchors()"
      ],
      "metadata": {
        "id": "eC34RJP3E7UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'trained1.pth'\n",
        "file_path1 = 'trained2.pth'\n",
        "modello= torch.load(file_path)\n",
        "rp=RPN(2048,540)\n",
        "rp.load_state_dict(modello)\n",
        "FE=FeatureExtractor()\n",
        "modello=torch.load(file_path1)\n",
        "fast=FASTRCNN1()\n",
        "fast.load_state_dict(modello)\n",
        "\n"
      ],
      "metadata": {
        "id": "nviaIGfGE-Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8CjrraxSfKZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FE=FeatureExtractor()\n",
        "rp=RPN(2048,540)\n",
        "\n",
        "obj_criterion = nn.BCEWithLogitsLoss()\n",
        "bbox_criterion = nn.SmoothL1Loss()\n",
        "\n",
        "optimizer = optim.Adam(rp.parameters(), lr=0.01)\n",
        "num_epochs=30\n",
        "losses=[]\n",
        "losses1=[]\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
        "    lp=0\n",
        "    for i, data in enumerate(dataloader):\n",
        "        images, boxes, classes = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calcola cls_targets e reg_targets per ogni batch di dati\n",
        "        cls_targets, reg_targets = compute_targets(images, anchors, boxes)\n",
        "        if reg_targets is None:\n",
        "            continue\n",
        "\n",
        "        # Esegui il modello RPN sulle immagini\n",
        "        features = FE(images)\n",
        "        obj_scores, reg_scores = rp(features)\n",
        "        # Converti obj_scores in long\n",
        "        cls_targets=cls_targets.float()\n",
        "\n",
        "        # Calcola le losses\n",
        "\n",
        "        cls_loss = obj_criterion(obj_scores, cls_targets)\n",
        "        reg_loss = bbox_criterion(reg_scores, reg_targets)\n",
        "        total_loss = cls_loss + reg_loss\n",
        "        losses.append(total_loss.detach().cpu().item())\n",
        "\n",
        "        # Esegui la retropropagazione\n",
        "        total_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item()}')\n",
        "\n",
        "        lp=lp+total_loss\n",
        "    lp=lp/10\n",
        "    losses1.append(lp.detach().cpu().item())\n",
        "    print('lista2: ',losses1)"
      ],
      "metadata": {
        "id": "Rv8F2Guze6Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast=FastRCNN1()\n",
        "\n",
        "obj_criterion = nn.CrossEntropyLoss()\n",
        "bbox_criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(fast.parameters(), lr=0.001)\n",
        "num_epochs=10\n",
        "\n",
        "losses=[]\n",
        "losses1=[]\n",
        "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
        "    pl=0\n",
        "    for i, data in enumerate(dataloader):\n",
        "        images, boxes, classes = data\n",
        "        classes=classes.reshape(-1)\n",
        "        classes = classes.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = FE(images)\n",
        "        obj_scores, reg_scores = rp(features)\n",
        "        proposals=get_proposals(obj_scores,reg_scores)\n",
        "\n",
        "        cl,bb=fast(proposals)\n",
        "\n",
        "        cls_loss = obj_criterion(cl, classes)\n",
        "        reg_loss = bbox_criterion(bb, boxes)\n",
        "        total_loss = cls_loss/4 + reg_loss/9\n",
        "        losses.append((total_loss).detach().cpu().item())\n",
        "\n",
        "        pl=pl+total_loss\n",
        "        total_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    pl=pl/10\n",
        "    losses1.append((pl).detach().cpu().item())\n",
        "    print('mean: ',pl)"
      ],
      "metadata": {
        "id": "v2-RNbkULYWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}